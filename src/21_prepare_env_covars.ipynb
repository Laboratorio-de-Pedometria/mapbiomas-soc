{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title: SoilData - Soil Organic Carbon Stock\n",
    "# subtitle: Prepare environmental covariates\n",
    "# author: Alessandro Samuel-Rosa and Taciara Zborowski Horst\n",
    "# data: 2024 CC-BY\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Identify working directory, saving the path to a variable\n",
    "src_dir = os.getcwd()\n",
    "work_dir = os.path.dirname(src_dir)\n",
    "\n",
    "# Read the TXT file '20_soildata_soc.txt' with the soil data from the 'data' folder\n",
    "# Field separator: tab\n",
    "file_path = os.path.join(work_dir, 'data', '20_soildata_soc.txt')\n",
    "soildata_df = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "print(soildata_df.shape)\n",
    "\n",
    "# Read the TXT file '21_soildata_soc.txt' with the coordinates from the 'data' folder\n",
    "# Field separator: tab. Decimal separator: comma\n",
    "file_path = os.path.join(work_dir, 'data', '21_soildata_soc.txt')\n",
    "df2 = pd.read_csv(file_path, sep='\\t', low_memory=False)\n",
    "print(df2.shape)\n",
    "\n",
    "# Compare the two data frames to check if there were changes in the data\n",
    "# Use equals() method to compare the two data frames\n",
    "# The result is a boolean value, True if the data frames are equal, False otherwise\n",
    "# We consider only the following variables in the comparison:\n",
    "# dataset_id, observacao_id, coord_x, coord_y, data_ano\n",
    "df1 = soildata_df[['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']]\n",
    "df2 = df2[['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']]\n",
    "if df1.equals(df2):\n",
    "    print(\"The DataFrames are identical.\")\n",
    "else:\n",
    "    print(\"The DataFrames are different.\")\n",
    "del df1, df2\n",
    "# 29683 layers (GET FROM PREVIOUS SCRIPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out soil layers without geographic coordinates\n",
    "# coord_x and coord_y are the columns with the geographic coordinates\n",
    "soildata_xy = soildata_df[soildata_df['coord_x'].notnull()]\n",
    "soildata_xy = soildata_xy[soildata_xy['coord_y'].notnull()]\n",
    "\n",
    "# Remove all duplicates based on the following columns:\n",
    "# \"dataset_id\", \"observacao_id\", \"coord_x\", \"coord_y\", \"data_ano\"\n",
    "# The first occurrence is kept, and the others are removed\n",
    "soildata_xy = soildata_xy[['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']]\n",
    "soildata_xy = soildata_xy.drop_duplicates()\n",
    "target = 13292 # GET VALUE FROM PREVIOUS SCRIPT!\n",
    "print(\n",
    "  'There should be', target, 'events:', target == soildata_xy.shape[0],\n",
    "  '\\nThere are', soildata_xy.shape[0], 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the data frame\n",
    "print(soildata_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import ee\n",
    "import geemap\n",
    "\n",
    "# GEE user authentication\n",
    "ee.Authenticate()\n",
    "\n",
    "# Initialize the Earth Engine API\n",
    "ee.Initialize()\n",
    "\n",
    "# Convert DataFrame to Earth Engine Feature Collection\n",
    "soildata_fc = geemap.df_to_ee(soildata_xy, latitude = 'coord_y', longitude = 'coord_x')\n",
    "print(soildata_fc.size().getInfo(), 'features')\n",
    "\n",
    "# Function to split a feature collection (sampling points) into chunks\n",
    "def split_sampling_points(fc, chunk_size):\n",
    "    features = fc.toList(fc.size())\n",
    "    chunks = [features.slice(i, i + chunk_size) for i in range(0, features.size().getInfo(), chunk_size)]\n",
    "    return [ee.FeatureCollection(chunk) for chunk in chunks]\n",
    "\n",
    "# Split the sample points into subsets of 1000 points each\n",
    "# This is necessary to avoid timeout errors\n",
    "chunk_size = 1000\n",
    "soildata_chunks = split_sampling_points(soildata_fc, chunk_size)\n",
    "print(len(soildata_chunks), 'chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil Grids 250m v2.0\n",
    "# (this takes about 10 minutes to run)\n",
    "\n",
    "# Soil Grids 250m v2.0: bdod_mean (bulk density)\n",
    "bdod_image = ee.Image(\"projects/soilgrids-isric/bdod_mean\")\n",
    "bdod_image = bdod_image.select(['bdod_0-5cm_mean', 'bdod_5-15cm_mean', 'bdod_15-30cm_mean'])\n",
    "\n",
    "# Soil Grids 250m v2.0: clay_mean (clay content)\n",
    "clay_image = ee.Image(\"projects/soilgrids-isric/clay_mean\")\n",
    "clay_image = clay_image.select(['clay_0-5cm_mean', 'clay_5-15cm_mean', 'clay_15-30cm_mean'])\n",
    "\n",
    "# Soil Grids 250m v2.0: sand_mean (sand content)\n",
    "sand_image = ee.Image(\"projects/soilgrids-isric/sand_mean\")\n",
    "sand_image = sand_image.select(['sand_0-5cm_mean', 'sand_5-15cm_mean', 'sand_15-30cm_mean'])\n",
    "\n",
    "# Soil Grids 250m v2.0: soc_mean (soil organic carbon)\n",
    "soc_image = ee.Image(\"projects/soilgrids-isric/soc_mean\")\n",
    "soc_image = soc_image.select(['soc_0-5cm_mean', 'soc_5-15cm_mean', 'soc_15-30cm_mean'])\n",
    "\n",
    "# Soil Grids 250m v2.0: cfvo_mean (coarse fragments volume)\n",
    "cfvo_image = ee.Image(\"projects/soilgrids-isric/cfvo_mean\")\n",
    "cfvo_image = cfvo_image.select(['cfvo_0-5cm_mean', 'cfvo_5-15cm_mean', 'cfvo_15-30cm_mean'])\n",
    "\n",
    "# Stack the images into a single multiband image\n",
    "soilgrids_image = bdod_image.addBands([clay_image, sand_image, soc_image, cfvo_image])\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop over each subset and sample the data\n",
    "for chunk in soildata_chunks:\n",
    "    sampled_points = geemap.extract_values_to_points(chunk, soilgrids_image, scale=250)\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    dataframes.append(sampled_df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "soilgrids_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "# Replace dash with underscores\n",
    "soilgrids_df.columns = soilgrids_df.columns.str.replace('-', '_')\n",
    "# Remove _mean from column names\n",
    "soilgrids_df.columns = soilgrids_df.columns.str.replace('_mean', '')\n",
    "\n",
    "# Print column names\n",
    "print(soilgrids_df.columns)\n",
    "\n",
    "# Check if number of rows of soildata_df is the same as that of soildata_xy\n",
    "# If the number of rows is the same, the merge was successful\n",
    "target = soildata_xy.shape[0]\n",
    "print(\n",
    "  '\\nThere should be', target, 'events:',\n",
    "  target == soilgrids_df.shape[0], '\\nThere are', soilgrids_df.shape[0], 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics of the SoilGrids data\n",
    "summary = soilgrids_df.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# MapBiomas LULC Collection\n",
    "# (this takes about 10 minutes to run)\n",
    "\n",
    "# Import the MapBiomas Collection 9.0\n",
    "collection = 'projects/mapbiomas-public/assets/brazil/lulc/collection9/mapbiomas_collection90_integration_v1'\n",
    "mapbiomas_image = ee.Image(collection)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop over each subset and sample the data\n",
    "for chunk in soildata_chunks:\n",
    "    sampled_points = geemap.extract_values_to_points(chunk, mapbiomas_image, scale=30)\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    dataframes.append(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all DataFrames into a single DataFrame\n",
    "mapbiomas_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "# Remove classification_ prefix from column names\n",
    "mapbiomas_df.columns = mapbiomas_df.columns.str.replace('classification_', '')\n",
    "\n",
    "# Get LULC class at the year of sampling (data_ano)\n",
    "# Each column in the MapBiomas dataset represents a year. The column name is the year of the\n",
    "# classification. The value is the class code. We need to extract the class code for the year of\n",
    "# sampling, which is stored in the data_ano column.\n",
    "# Step 1: Create a new column YEAR based on data_ano\n",
    "mapbiomas_df['YEAR'] = mapbiomas_df['data_ano']\n",
    "# Step 2: If YEAR is less than 1985, set it to 0 (no data)\n",
    "mapbiomas_df.loc[mapbiomas_df['YEAR'] < 1985, 'YEAR'] = 0\n",
    "# Step 3: Find the column index for each YEAR\n",
    "lulc_idx = mapbiomas_df.columns.get_indexer(mapbiomas_df['YEAR'].astype(str))\n",
    "# Step 4: Extract the class code for each row based on the data_ano column\n",
    "lulc = mapbiomas_df.to_numpy()\n",
    "lulc = lulc[range(len(lulc)), lulc_idx]\n",
    "# Step 5: Convert the extracted class codes to strings and assign them to a new column lulc\n",
    "mapbiomas_df['lulc'] = lulc.astype(str)\n",
    "# Step 6: Drop the YEAR column\n",
    "mapbiomas_df = mapbiomas_df.drop(columns = ['YEAR'])\n",
    "\n",
    "# Some columns of mapbiomas_df are named with the year of the classification, ranging from 1985 to\n",
    "# the present year. This columns need to be dropped.\n",
    "# Drop columns with years as column names\n",
    "current_year = datetime.now().year\n",
    "years_to_drop = [str(year) for year in range(1985, current_year + 1)]\n",
    "mapbiomas_df.drop(columns=years_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Reclassify the land cover classes\n",
    "forest_codes = ['1.0', '3.0', '4.0', '5.0', '6.0', '49.0']\n",
    "nonforest_codes = ['10.0', '11.0', '12.0', '32.0', '29.0', '50.0']\n",
    "pasture_codes = ['15.0']\n",
    "agriculture_codes = [\n",
    "    '14.0', '18.0', '19.0', '39.0', '20.0', '40.0', '62.0', '41.0', '36.0', '46.0', '47.0', '48.0',\n",
    "    '21.0', '35.0']\n",
    "forestry_codes = ['9.0']\n",
    "nonvegetation_codes = ['22.0', '23.0', '24.0', '30.0', '25.0', '26.0', '33.0', '31.0', '27.0']\n",
    "na_codes = ['0', '0.0', 'nan']\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(forest_codes, 'forest')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(nonforest_codes, 'nonforest')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(pasture_codes, 'pasture')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(agriculture_codes, 'agriculture')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(forestry_codes, 'forestry')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(nonvegetation_codes, 'nonvegetation')\n",
    "mapbiomas_df['lulc'] = mapbiomas_df['lulc'].replace(na_codes, pd.NA)\n",
    "\n",
    "# Print summary of the land cover classes, including NA\n",
    "print('\\nDistribution of land cover/land use classes (including NA):')\n",
    "print(mapbiomas_df['lulc'].value_counts(dropna=False))\n",
    "\n",
    "# Check if number of rows of mapbiomas_df is the same as that of soildata_xy\n",
    "# If the number of rows is the same, the sampling was successful\n",
    "target = soildata_xy.shape[0]\n",
    "print(\n",
    "  '\\nThere should be', target, 'events:',\n",
    "  target == mapbiomas_df.shape[0], '\\nThere are', mapbiomas_df.shape[0], 'events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geomorphometry 90 m\n",
    "\n",
    "# Sample the terrain attributes\n",
    "# projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/OT_GEOMORPHOMETRY_90m\n",
    "# (this takes about 10 minutes to run)\n",
    "dem = ee.Image(\"projects/mapbiomas-workspace/SOLOS/COVARIAVEIS/OT_GEOMORPHOMETRY_90m\")\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dem_dfs = []\n",
    "\n",
    "# Loop over each subset and sample the data\n",
    "for chunk in soildata_chunks:\n",
    "    sampled_points = geemap.extract_values_to_points(chunk, dem, scale=90)\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    dem_dfs.append(sampled_df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "dem_dataframe = pd.concat(dem_dfs, ignore_index=True)\n",
    "\n",
    "# print column names\n",
    "print(dem_dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics of the Geomorphometry data\n",
    "summary = dem_dataframe.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data sampled from SoilGrids, MapBiomas, and Geomorphometry into soildata_df\n",
    "# Keep all rows from soildata_df\n",
    "merge_columns = ['dataset_id', 'observacao_id', 'coord_x', 'coord_y', 'data_ano']\n",
    "output_df = soildata_df.merge(soilgrids_df, on = merge_columns, how = 'left')\n",
    "output_df = output_df.merge(mapbiomas_df, on = merge_columns, how = 'left')\n",
    "output_df = output_df.merge(dem_dataframe, on = merge_columns, how = 'left')\n",
    "\n",
    "# # Fill empty cells in the 'lulc' column with 'unknown'\n",
    "# output_df['lulc'] = output_df['lulc'].fillna('unknown')\n",
    "\n",
    "# Check if number of rows of output_df is the same as that of soildata_df\n",
    "# If the number of rows is the same, the merge was successful\n",
    "target = soildata_df.shape[0]\n",
    "print(\n",
    "  '\\nThere should be', target, 'layers:',\n",
    "  target == output_df.shape[0], '\\nThere are', output_df.shape[0], 'layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the output_df to a TXT file\n",
    "# Field separator: tab. Decimal separator: comma\n",
    "file_path = os.path.join(work_dir, 'data', '21_soildata_soc.txt')\n",
    "output_df.to_csv(file_path, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geemap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
